{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 15,
   "source": [
    "import os, sys\n",
    "import logging\n",
    "import argparse\n",
    "import csv\n",
    "import gc\n",
    "from tqdm import tqdm\n",
    "from collections import OrderedDict, Counter\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import classification_report\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.nn.utils import clip_grad_norm_\n",
    "from torch.utils.data import DataLoader\n",
    "from torch.optim.lr_scheduler import LambdaLR\n",
    "from transformers import BertTokenizer, BertModel, BertForSequenceClassification\n",
    "from transformers import AdamW, get_scheduler, get_linear_schedule_with_warmup, get_cosine_schedule_with_warmup\n",
    "from transformers import WEIGHTS_NAME, CONFIG_NAME\n",
    "\n",
    "from CustomDataset import MovieDataset\n",
    "from CustomModel import SentimentBertModel\n",
    "from util.log import setup_default_logging\n",
    "from util.metrics import AverageMeter\n",
    "from util.collate import customCollate\n",
    "\n",
    "try:\n",
    "    import wandb\n",
    "    has_wandb = True\n",
    "except ImportError: \n",
    "    has_wandb = False"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "source": [
    "_logger = logging.getLogger('train')\n",
    "parser = argparse.ArgumentParser(description='Train Config', add_help=False)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "source": [
    "def get_dataset(data_path, tokenizer, max_len, random_seed, is_train=True):\n",
    "    document, target = [], []\n",
    "    with open(data_path, 'r') as f:\n",
    "        lines = list(csv.reader(f, delimiter='\\t'))\n",
    "        header = lines.pop(0)\n",
    "        for line in lines:\n",
    "            document.append(line[1])\n",
    "            target.append(int(line[2]))\n",
    "    if is_train:\n",
    "        train_doc, valid_doc, train_target, valid_target = train_test_split(\n",
    "            document, target, test_size=0.1, shuffle=True, stratify=target, random_state=random_seed\n",
    "        )\n",
    "        train_dataset = MovieDataset(tokenizer, train_doc, train_target, max_len)\n",
    "        valid_dataset = MovieDataset(tokenizer, valid_doc, valid_target, max_len)\n",
    "        _logger.info(f'train dataset : {len(train_dataset)}, valid dataset : {len(valid_dataset)}')\n",
    "        _logger.info(f'train dataset : {Counter(train_target)}, valid dataset : {Counter(valid_target)}')\n",
    "        return train_dataset, valid_dataset\n",
    "    else:\n",
    "        test_dataset = MovieDataset(tokenizer, document, target, max_len)\n",
    "        _logger.info(f'test dataset :{len(test_dataset)}')\n",
    "        _logger.info(f'test dataset :{Counter(target)}')\n",
    "        return test_dataset"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "source": [
    "def initalize_model(prev_model, max_len, finetune=False):\n",
    "    model = SentimentBertModel.from_pretrained(prev_model, \n",
    "                                                n_classes=2,\n",
    "                                                max_length=max_len)\n",
    "    if finetune:\n",
    "        for param in model.parameters():\n",
    "            param.requires_grad = True\n",
    "    else:\n",
    "        for name, param in model.named_parameters():\n",
    "            if 'classifier' in name:\n",
    "                param.requires_grad = True\n",
    "            else:\n",
    "                param.requires_grad = False\n",
    "\n",
    "    return model"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "source": [
    "def train_one_epoch(model, loader, device, loss_fn, optimizer):\n",
    "    ### argument\n",
    "    log_interval = 1\n",
    "    ###\n",
    "\n",
    "    model.train()\n",
    "    \n",
    "    train_loss_m = AverageMeter()\n",
    "    last_idx = len(loader) - 1\n",
    "    for idx, batch in tqdm(enumerate(loader), total=len(loader)):\n",
    "        last_batch = idx == last_idx\n",
    "        optimizer.zero_grad()    \n",
    "        batch = {k: v.to(device) for k, v in batch.items()}\n",
    "        logits = model(input_ids=batch['input_ids'],\n",
    "                        token_type_ids=batch['token_type_ids'],\n",
    "                        attention_mask=batch['attention_mask'])\n",
    "        \n",
    "        loss = loss_fn(logits.view(-1, 2), batch['labels'].view(-1))\n",
    "        train_loss_m.update(loss.data.item(), batch['input_ids'].size(0))\n",
    "        # _logger.info(f'batch_train_loss : {loss.data.item()}')\n",
    "        loss.backward()\n",
    "\n",
    "        clip_grad_norm_(model.parameters(), 1.0)\n",
    "\n",
    "        optimizer.step()\n",
    "        \n",
    "\n",
    "        if last_batch or (idx+1 % log_interval == 0):\n",
    "            lrl = [param_group['lr'] for param_group in optimizer.param_groups]\n",
    "            avg_lr = sum(lrl)/len(lrl)\n",
    "            _logger.info(f'avg_train_loss : {train_loss_m.avg}, LR : {avg_lr}')\n",
    "\n",
    "        del batch, loss\n",
    "\n",
    "    del loader\n",
    "    gc.collect()\n",
    "\n",
    "    metrics = OrderedDict([('loss', train_loss_m.avg)])\n",
    "\n",
    "    return metrics"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "source": [
    "def train_one_epoch2(model, loader, device, loss_fn, optimizer):\n",
    "    ### argument\n",
    "    log_interval = 1\n",
    "    ###\n",
    "\n",
    "    model.train()\n",
    "    \n",
    "    train_loss_m = AverageMeter()\n",
    "    last_idx = len(loader) - 1\n",
    "    for idx, batch in tqdm(enumerate(loader), total=len(loader)):\n",
    "        last_batch = idx == last_idx\n",
    "        optimizer.zero_grad()    \n",
    "        batch = {k: v.to(device) for k, v in batch.items()}\n",
    "        outputs = model(input_ids=batch['input_ids'],\n",
    "                        token_type_ids=batch['token_type_ids'],\n",
    "                        attention_mask=batch['attention_mask'],\n",
    "                        labels=batch['labels'])\n",
    "        \n",
    "        loss = outputs[0]\n",
    "        train_loss_m.update(loss.data.item(), batch['input_ids'].size(0))\n",
    "        # _logger.info(f'batch_train_loss : {loss.data.item()}')\n",
    "        loss.backward()\n",
    "\n",
    "        clip_grad_norm_(model.parameters(), 1.0)\n",
    "\n",
    "        optimizer.step()\n",
    "        \n",
    "\n",
    "        if last_batch or (idx+1 % log_interval == 0):\n",
    "            lrl = [param_group['lr'] for param_group in optimizer.param_groups]\n",
    "            avg_lr = sum(lrl)/len(lrl)\n",
    "            _logger.info(f'avg_train_loss : {train_loss_m.avg}, LR : {avg_lr}')\n",
    "\n",
    "        del batch, loss\n",
    "\n",
    "    del loader\n",
    "    gc.collect()\n",
    "\n",
    "    metrics = OrderedDict([('loss', train_loss_m.avg)])\n",
    "\n",
    "    return metrics"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "source": [
    "def validation(model, loader, device, loss_fn):\n",
    "    ### argument\n",
    "    log_interval = 1\n",
    "    ###\n",
    "\n",
    "    model.eval()\n",
    "    val_loss_m = AverageMeter()\n",
    "    acc_m = AverageMeter()\n",
    "\n",
    "    last_idx = len(loader) - 1\n",
    "    with torch.no_grad():\n",
    "        for idx, batch in tqdm(enumerate(loader), total=len(loader)):\n",
    "            last_batch = idx == last_idx\n",
    "            batch = {k: v.to(device) for k, v in batch.items()}\n",
    "            logits = model(input_ids=batch['input_ids'],\n",
    "                                 token_type_ids=batch['token_type_ids'],\n",
    "                                 attention_mask=batch['attention_mask'])\n",
    "            \n",
    "            loss = loss_fn(logits.view(-1, 2), batch['labels'].view(-1))\n",
    "            val_loss_m.update(loss.data.item(), batch['input_ids'].size(0))\n",
    "            \n",
    "            acc = sum([i == j for i, j in zip(torch.argmax(logits, 1).tolist(), batch['labels'])]) / len(batch['labels'])\n",
    "            acc_m.update(acc)\n",
    "\n",
    "            del batch, loss, logits            \n",
    "\n",
    "        if last_batch or (idx+1 % log_interval == 0):\n",
    "            _logger.info(f'avg_val_loss : {val_loss_m.avg}, avg_accuracy : {acc_m.avg}')\n",
    "\n",
    "    metrics = OrderedDict([('loss', val_loss_m.avg), ('accuracy', acc_m.avg)])\n",
    "\n",
    "    del loader\n",
    "    gc.collect()\n",
    "\n",
    "    return metrics"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "source": [
    "def validation2(model, loader, device, loss_fn):\n",
    "    ### argument\n",
    "    log_interval = 1\n",
    "    ###\n",
    "\n",
    "    model.eval()\n",
    "    # val_loss_m = AverageMeter()\n",
    "    acc_m = AverageMeter()\n",
    "\n",
    "    last_idx = len(loader) - 1\n",
    "    with torch.no_grad():\n",
    "        for idx, batch in tqdm(enumerate(loader), total=len(loader)):\n",
    "            last_batch = idx == last_idx\n",
    "            batch = {k: v.to(device) for k, v in batch.items()}\n",
    "            outputs = model(input_ids=batch['input_ids'],\n",
    "                                 token_type_ids=batch['token_type_ids'],\n",
    "                                 attention_mask=batch['attention_mask'])\n",
    "            \n",
    "            logits = outputs[0]\n",
    "            # val_loss_m.update(loss.data.item(), batch['input_ids'].size(0))\n",
    "            \n",
    "            acc = sum([i == j for i, j in zip(torch.argmax(logits, 1).tolist(), batch['labels'])]) / len(batch['labels'])\n",
    "            acc_m.update(acc)\n",
    "\n",
    "            del batch, outputs            \n",
    "\n",
    "        if last_batch or (idx+1 % log_interval == 0):\n",
    "            _logger.info(f'avg_accuracy : {acc_m.avg}')\n",
    "\n",
    "    metrics = OrderedDict([('accuracy', acc_m.avg)])\n",
    "\n",
    "    del loader\n",
    "    gc.collect()\n",
    "\n",
    "    return metrics"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "source": [
    "def save(model, tokenizer, new_model):\n",
    "    _logger.info('saving model...')\n",
    "    os.makedirs(new_model, exist_ok=True)\n",
    "    torch.save(model.state_dict(), new_model + WEIGHTS_NAME)\n",
    "    model.config.to_json_file(new_model + CONFIG_NAME)\n",
    "    tokenizer.save_pretrained(new_model)\n",
    "    _logger.info(f'saved! {new_model}')"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "source": [
    "### argument\n",
    "experiment = \"sbse\"\n",
    "random_seed = 1234\n",
    "# prev_model = \"bert-base-multilingual-cased\"\n",
    "prev_model = \"monologg/kobert\"\n",
    "train_data_path = \"/home/ubuntu/workspace/kaist.sbse/proj/data/ratings_train.txt\"\n",
    "new_model = \"/home/ubuntu/workspace/kaist.sbse/proj/model/bert.v4/\"\n",
    "max_len = 512\n",
    "batch_size = 8\n",
    "lr = 5e-5\n",
    "epochs = 10\n",
    "valid_every_n_batch = 1\n",
    "save_best = \"loss\"\n",
    "val_metric = \"loss\"\n",
    "###"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "source": [
    "if has_wandb:\n",
    "        wandb.init(project=experiment)\n",
    "else: \n",
    "    _logger.warning(\"You've requested to log metrics to wandb but package not found. \"\n",
    "                    \"Metrics not being logged to wandb, try `pip install wandb`\")"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "Failed to detect the name of this notebook, you can set it manually with the WANDB_NOTEBOOK_NAME environment variable to enable code saving.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mhannabros\u001b[0m (use `wandb login --relogin` to force relogin)\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: wandb version 0.12.0 is available!  To upgrade, please run:\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:  $ pip install wandb --upgrade\n"
     ]
    },
    {
     "output_type": "display_data",
     "data": {
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ],
      "text/html": [
       "\n",
       "                Tracking run with wandb version 0.11.2<br/>\n",
       "                Syncing run <strong style=\"color:#cdcd00\">lyric-leaf-31</strong> to <a href=\"https://wandb.ai\" target=\"_blank\">Weights & Biases</a> <a href=\"https://docs.wandb.com/integrations/jupyter.html\" target=\"_blank\">(Documentation)</a>.<br/>\n",
       "                Project page: <a href=\"https://wandb.ai/hannabros/sbse\" target=\"_blank\">https://wandb.ai/hannabros/sbse</a><br/>\n",
       "                Run page: <a href=\"https://wandb.ai/hannabros/sbse/runs/3ngrlcpf\" target=\"_blank\">https://wandb.ai/hannabros/sbse/runs/3ngrlcpf</a><br/>\n",
       "                Run data is saved locally in <code>/home/ubuntu/workspace/kaist.sbse/proj/wandb/run-20210811_172317-3ngrlcpf</code><br/><br/>\n",
       "            "
      ]
     },
     "metadata": {}
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "source": [
    "setup_default_logging()\n",
    "if torch.cuda.is_available():\n",
    "    device = torch.device(\"cuda\")\n",
    "    _logger.info(f'GPU: {torch.cuda.get_device_name(0)}')\n",
    "else:\n",
    "    device = torch.device(\"cpu\")\n",
    "torch.manual_seed(random_seed)"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "GPU: Tesla T4\n"
     ]
    },
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "<torch._C.Generator at 0x7f6f7e8fd330>"
      ]
     },
     "metadata": {},
     "execution_count": 12
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "source": [
    "tokenizer = BertTokenizer.from_pretrained(prev_model, do_lower_case=False)\n",
    "train_dataset, valid_dataset = get_dataset(train_data_path, tokenizer, max_len, random_seed, is_train=True)\n",
    "\n",
    "train_loader = DataLoader(train_dataset, batch_size=batch_size, collate_fn=customCollate)\n",
    "valid_loader = DataLoader(valid_dataset, batch_size=batch_size, collate_fn=customCollate)"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "Lock 140116741901968 acquired on /home/ubuntu/.cache/huggingface/transformers/efee434f5f4c5c89b5a7d8d5f30bbb0496f1540349fcfa21729cec5b96cfd2d1.719459e20bc981bc2093e859b02c3a3e51bab724d6b58927b23b512a3981229f.lock\n"
     ]
    },
    {
     "output_type": "display_data",
     "data": {
      "text/plain": [
       "Downloading:   0%|          | 0.00/77.8k [00:00<?, ?B/s]"
      ],
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "4e9dfecaea0947ca895f1f4100dfb95b"
      }
     },
     "metadata": {}
    },
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "Lock 140116741901968 released on /home/ubuntu/.cache/huggingface/transformers/efee434f5f4c5c89b5a7d8d5f30bbb0496f1540349fcfa21729cec5b96cfd2d1.719459e20bc981bc2093e859b02c3a3e51bab724d6b58927b23b512a3981229f.lock\n",
      "Lock 140116734508880 acquired on /home/ubuntu/.cache/huggingface/transformers/d1c07e179f5e00959a3c8e4a150eaa4907dfe26544e4a71f2b0163982a476523.767d1b760a83978bae6c324157fad57ee513af333a7cea6986e852579f6f0dd1.lock\n"
     ]
    },
    {
     "output_type": "display_data",
     "data": {
      "text/plain": [
       "Downloading:   0%|          | 0.00/51.0 [00:00<?, ?B/s]"
      ],
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "c84cfa9be66f4a2ca2e0fd89053914a8"
      }
     },
     "metadata": {}
    },
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "Lock 140116734508880 released on /home/ubuntu/.cache/huggingface/transformers/d1c07e179f5e00959a3c8e4a150eaa4907dfe26544e4a71f2b0163982a476523.767d1b760a83978bae6c324157fad57ee513af333a7cea6986e852579f6f0dd1.lock\n",
      "Lock 140116734245456 acquired on /home/ubuntu/.cache/huggingface/transformers/31dc8da633439f22ed80bede01f337996bc709eb8429f86f2b24e2103558b039.89a06cdfd16840fd89cc5c2493ef63cd0b6068e85f70ac988a3673e2722cab2e.lock\n"
     ]
    },
    {
     "output_type": "display_data",
     "data": {
      "text/plain": [
       "Downloading:   0%|          | 0.00/426 [00:00<?, ?B/s]"
      ],
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "001ee832b1164274acf14aaf9ca048bd"
      }
     },
     "metadata": {}
    },
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "Lock 140116734245456 released on /home/ubuntu/.cache/huggingface/transformers/31dc8da633439f22ed80bede01f337996bc709eb8429f86f2b24e2103558b039.89a06cdfd16840fd89cc5c2493ef63cd0b6068e85f70ac988a3673e2722cab2e.lock\n",
      "train dataset : 135000, valid dataset : 15000\n",
      "train dataset : Counter({0: 67656, 1: 67344}), valid dataset : Counter({0: 7517, 1: 7483})\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "source": [
    "model = initalize_model(prev_model, max_len)\n",
    "model = model.to(device)"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "Lock 140116790583376 acquired on /home/ubuntu/.cache/huggingface/transformers/9525d6f96682baa1f21538ea58d36263fe16a46345dd9637e3e28a4df2f9380f.ebe6e13ff204bebbffd4764cda3d5a97dc690a9c4110bde6d909ddc3ed5c4585.lock\n"
     ]
    },
    {
     "output_type": "display_data",
     "data": {
      "text/plain": [
       "Downloading:   0%|          | 0.00/369M [00:00<?, ?B/s]"
      ],
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "498a5cbd63a84195b98b1410a6cd2167"
      }
     },
     "metadata": {}
    },
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "Lock 140116790583376 released on /home/ubuntu/.cache/huggingface/transformers/9525d6f96682baa1f21538ea58d36263fe16a46345dd9637e3e28a4df2f9380f.ebe6e13ff204bebbffd4764cda3d5a97dc690a9c4110bde6d909ddc3ed5c4585.lock\n",
      "Some weights of SentimentBertModel were not initialized from the model checkpoint at monologg/kobert and are newly initialized: ['classifier.0.weight', 'classifier.0.bias', 'classifier.2.bias', 'classifier.2.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "source": [
    "### Sequence Classification\n",
    "model = BertForSequenceClassification.from_pretrained(prev_model, num_labels=2)\n",
    "\n",
    "for name, param in model.named_parameters():\n",
    "    if 'classifier' in name:\n",
    "        param.requires_grad = True\n",
    "    else:\n",
    "        param.requires_grad = False\n",
    "\n",
    "model.to(device)"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "Some weights of the model checkpoint at bert-base-multilingual-cased were not used when initializing BertForSequenceClassification: ['cls.predictions.transform.LayerNorm.weight', 'cls.predictions.bias', 'cls.seq_relationship.weight', 'cls.predictions.transform.dense.weight', 'cls.predictions.decoder.weight', 'cls.seq_relationship.bias', 'cls.predictions.transform.dense.bias', 'cls.predictions.transform.LayerNorm.bias']\n",
      "- This IS expected if you are initializing BertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-multilingual-cased and are newly initialized: ['classifier.weight', 'classifier.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "BertForSequenceClassification(\n",
       "  (bert): BertModel(\n",
       "    (embeddings): BertEmbeddings(\n",
       "      (word_embeddings): Embedding(119547, 768, padding_idx=0)\n",
       "      (position_embeddings): Embedding(512, 768)\n",
       "      (token_type_embeddings): Embedding(2, 768)\n",
       "      (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "      (dropout): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "    (encoder): BertEncoder(\n",
       "      (layer): ModuleList(\n",
       "        (0): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (1): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (2): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (3): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (4): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (5): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (6): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (7): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (8): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (9): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (10): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (11): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (pooler): BertPooler(\n",
       "      (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "      (activation): Tanh()\n",
       "    )\n",
       "  )\n",
       "  (dropout): Dropout(p=0.1, inplace=False)\n",
       "  (classifier): Linear(in_features=768, out_features=2, bias=True)\n",
       ")"
      ]
     },
     "metadata": {},
     "execution_count": 27
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "source": [
    "total_steps = len(train_loader) * epochs\n",
    "optimizer = AdamW(filter(lambda p: p.requires_grad, model.parameters()), lr=lr)\n",
    "# scheduler = get_linear_schedule_with_warmup(optimizer, num_warmup_steps=2, num_training_steps=total_steps)\n",
    "# scheduler = LambdaLR(optimizer=optimizer, lr_lambda=lambda epoch: 0.9 ** epoch)\n",
    "warmup_step = int(total_steps * 0.1)\n",
    "scheduler = get_cosine_schedule_with_warmup(optimizer, num_warmup_steps=warmup_step, num_training_steps=total_steps)\n",
    "loss_fn = nn.CrossEntropyLoss()"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "source": [
    "min_val_loss = 10.0\n",
    "max_accuracy = 0.0\n",
    "\n",
    "try:\n",
    "    for epoch in range(epochs):\n",
    "        rowd = OrderedDict(epoch=epoch)\n",
    "        gc.collect()\n",
    "        _logger.info(f'Train {epoch} epoch')\n",
    "        train_metrics = train_one_epoch(model, train_loader, device, loss_fn, optimizer)\n",
    "        scheduler.step()\n",
    "        rowd.update([('train_' + k, v) for k, v in train_metrics.items()])\n",
    "        gc.collect()\n",
    "\n",
    "        if (epoch + 1) % valid_every_n_batch == 0:\n",
    "            _logger.info('validation ...')\n",
    "            valid_metrics = validation(model, valid_loader, device, loss_fn)\n",
    "            rowd.update([('eval_' + k, v) for k, v in valid_metrics.items()])\n",
    "            # scheduler.step()\n",
    "\n",
    "            gc.collect()\n",
    "\n",
    "            # Save / Early Stop\n",
    "            if save_best.lower().startswith('loss'):\n",
    "                _logger.info(f'best loss was {min_val_loss}')\n",
    "                if valid_metrics[val_metric] < min_val_loss:\n",
    "                    min_val_loss = valid_metrics[val_metric]\n",
    "                    _logger.info(f'best loss changed to {min_val_loss}')\n",
    "                    save(model, tokenizer, new_model)\n",
    "            elif save_best.lower().startswith('accuracy'):\n",
    "                _logger.info(f'best accuracy was {max_accuracy}')                    \n",
    "                if valid_metrics[val_metric] > max_accuracy:\n",
    "                    max_accuracy = valid_metrics[val_metric]\n",
    "                    _logger.info(f'best accuracy changed to {max_accuracy}')\n",
    "                    save(model, tokenizer, new_model)\n",
    "        wandb.log(rowd)\n",
    "\n",
    "except KeyboardInterrupt:\n",
    "    if save_best.lower().startswith('accuracy'):\n",
    "        _logger.info(f'Model accuracy was {max_accuracy}')\n",
    "    elif save_best.lower().startswith('loss'):\n",
    "        _logger.info(f'Model valid loss was {min_val_loss}')\n",
    "    elif save_best.lower().startswith('weight'):\n",
    "        _logger.info(f'Model weighted valid loss was {min_val_loss}')\n",
    "    _logger.info('Bye!')\n",
    "    "
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "Train 0 epoch\n",
      "  0%|          | 0/16875 [00:00<?, ?it/s]/home/ubuntu/anaconda3/envs/sbse/lib/python3.7/site-packages/transformers/tokenization_utils_base.py:2190: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
      "  FutureWarning,\n",
      "avg_train_loss : 0.7073715925216675, LR : 0.0\n",
      "100%|█████████▉| 16874/16875 [1:23:55<00:00,  3.35it/s]avg_train_loss : 0.6945980079191703, LR : 0.0\n",
      "100%|██████████| 16875/16875 [1:23:55<00:00,  3.35it/s]\n",
      "validation ...\n",
      "100%|██████████| 1875/1875 [08:33<00:00,  3.65it/s]\n",
      "avg_val_loss : 0.6944197644233704, avg_accuracy : 0.49406668543815613\n",
      "best loss was 10.0\n",
      "best loss changed to 0.6944197644233704\n",
      "saving model...\n",
      "saved! /home/ubuntu/workspace/kaist.sbse/proj/model/bert.v4/\n",
      "Train 1 epoch\n",
      "  0%|          | 0/16875 [00:00<?, ?it/s]avg_train_loss : 0.7007497549057007, LR : 2.9629629629629633e-09\n",
      "100%|█████████▉| 16874/16875 [1:24:04<00:00,  3.35it/s]avg_train_loss : 0.6943976277210094, LR : 2.9629629629629633e-09\n",
      "100%|██████████| 16875/16875 [1:24:05<00:00,  3.34it/s]\n",
      "validation ...\n",
      "100%|██████████| 1875/1875 [08:34<00:00,  3.64it/s]\n",
      "avg_val_loss : 0.6941889005343119, avg_accuracy : 0.4939333498477936\n",
      "best loss was 0.6944197644233704\n",
      "best loss changed to 0.6941889005343119\n",
      "saving model...\n",
      "saved! /home/ubuntu/workspace/kaist.sbse/proj/model/bert.v4/\n",
      "Train 2 epoch\n",
      "  0%|          | 0/16875 [00:00<?, ?it/s]avg_train_loss : 0.697275698184967, LR : 5.9259259259259265e-09\n",
      "100%|█████████▉| 16874/16875 [1:24:06<00:00,  3.34it/s]avg_train_loss : 0.6941659879825733, LR : 5.9259259259259265e-09\n",
      "100%|██████████| 16875/16875 [1:24:06<00:00,  3.34it/s]\n",
      "validation ...\n",
      "100%|██████████| 1875/1875 [08:34<00:00,  3.65it/s]\n",
      "avg_val_loss : 0.693701003074646, avg_accuracy : 0.49613332748413086\n",
      "best loss was 0.6941889005343119\n",
      "best loss changed to 0.693701003074646\n",
      "saving model...\n",
      "saved! /home/ubuntu/workspace/kaist.sbse/proj/model/bert.v4/\n",
      "Train 3 epoch\n",
      "  0%|          | 0/16875 [00:00<?, ?it/s]avg_train_loss : 0.7075254917144775, LR : 8.88888888888889e-09\n",
      "100%|█████████▉| 16874/16875 [1:24:03<00:00,  3.35it/s]avg_train_loss : 0.6936789462230823, LR : 8.88888888888889e-09\n",
      "100%|██████████| 16875/16875 [1:24:04<00:00,  3.35it/s]\n",
      "validation ...\n",
      "100%|██████████| 1875/1875 [08:34<00:00,  3.65it/s]\n",
      "avg_val_loss : 0.6930429856618245, avg_accuracy : 0.5007333159446716\n",
      "best loss was 0.693701003074646\n",
      "best loss changed to 0.6930429856618245\n",
      "saving model...\n",
      "saved! /home/ubuntu/workspace/kaist.sbse/proj/model/bert.v4/\n",
      "Train 4 epoch\n",
      "  0%|          | 0/16875 [00:00<?, ?it/s]avg_train_loss : 0.6898726224899292, LR : 1.1851851851851853e-08\n",
      "100%|█████████▉| 16874/16875 [1:24:02<00:00,  3.34it/s]avg_train_loss : 0.6931263192671316, LR : 1.1851851851851853e-08\n",
      "100%|██████████| 16875/16875 [1:24:03<00:00,  3.35it/s]\n",
      "validation ...\n",
      "100%|██████████| 1875/1875 [08:34<00:00,  3.65it/s]\n",
      "avg_val_loss : 0.6922849662144979, avg_accuracy : 0.5059999823570251\n",
      "best loss was 0.6930429856618245\n",
      "best loss changed to 0.6922849662144979\n",
      "saving model...\n",
      "saved! /home/ubuntu/workspace/kaist.sbse/proj/model/bert.v4/\n",
      "Train 5 epoch\n",
      "  0%|          | 0/16875 [00:00<?, ?it/s]avg_train_loss : 0.6822997331619263, LR : 1.4814814814814816e-08\n",
      " 70%|███████   | 11881/16875 [59:09<24:51,  3.35it/s]\n",
      "Model valid loss was 0.6922849662144979\n",
      "Bye!\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "source": [
    "### argument\n",
    "test_data_path = \"/home/ubuntu/workspace/kaist.sbse/proj/data/ratings_test.txt\"\n",
    "model_path = \"/home/ubuntu/workspace/kaist.sbse/proj/model/bert.v2/\"\n",
    "###"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "source": [
    "test_dataset = get_dataset(test_data_path, tokenizer, max_len, random_seed, is_train=False)\n",
    "test_loader = DataLoader(test_dataset, batch_size=batch_size, collate_fn=customCollate)"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "test dataset :1000\n",
      "test dataset :Counter({1: 25173, 0: 24827})\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "source": [
    "acc_m = AverageMeter()\n",
    "pred_ids, true_ids = [], []\n",
    "last_idx = len(test_loader) - 1\n",
    "model.eval()\n",
    "with torch.no_grad():\n",
    "    for idx, batch in tqdm(enumerate(test_loader), total=len(test_loader)):\n",
    "        last_batch = idx == last_idx\n",
    "        batch = {k: v.to(device) for k, v in batch.items()}\n",
    "        logits = model(input_ids=batch['input_ids'],\n",
    "                                 token_type_ids=batch['token_type_ids'],\n",
    "                                 attention_mask=batch['attention_mask'])\n",
    "            \n",
    "        loss = loss_fn(logits.view(-1, 2), batch['labels'].view(-1))\n",
    "            \n",
    "        acc = sum([i == j for i, j in zip(torch.argmax(logits, 1).tolist(), batch['labels'])]) / len(batch['labels'])\n",
    "        acc_m.update(acc)\n",
    "\n",
    "        pred_ids.extend(torch.argmax(logits, 1).tolist())\n",
    "        true_ids.extend(batch['labels'].tolist())\n",
    "\n",
    "        if last_batch:\n",
    "            _logger.info(f'avg_accuracy : {acc_m.avg}')"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "  0%|          | 0/125 [00:00<?, ?it/s]\n"
     ]
    },
    {
     "output_type": "error",
     "ename": "AttributeError",
     "evalue": "'SequenceClassifierOutput' object has no attribute 'view'",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_3207/1561665679.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     11\u001b[0m                                  attention_mask=batch['attention_mask'])\n\u001b[1;32m     12\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 13\u001b[0;31m         \u001b[0mloss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mloss_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlogits\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mview\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'labels'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mview\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     14\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     15\u001b[0m         \u001b[0macc\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msum\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0mj\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mj\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mzip\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0margmax\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlogits\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtolist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'labels'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m/\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatch\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'labels'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'SequenceClassifierOutput' object has no attribute 'view'"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "source": [
    "acc_m = AverageMeter()\n",
    "pred_ids, true_ids = [], []\n",
    "last_idx = len(test_loader) - 1\n",
    "model.eval()\n",
    "with torch.no_grad():\n",
    "    for idx, batch in tqdm(enumerate(test_loader), total=len(test_loader)):\n",
    "        last_batch = idx == last_idx\n",
    "        batch = {k: v.to(device) for k, v in batch.items()}\n",
    "        outputs = model(input_ids=batch['input_ids'],\n",
    "                                 token_type_ids=batch['token_type_ids'],\n",
    "                                 attention_mask=batch['attention_mask'])\n",
    "        logits = outputs[0]\n",
    "            \n",
    "        acc = sum([i == j for i, j in zip(torch.argmax(logits, 1).tolist(), batch['labels'])]) / len(batch['labels'])\n",
    "        acc_m.update(acc)\n",
    "\n",
    "        pred_ids.extend(torch.argmax(logits, 1).tolist())\n",
    "        true_ids.extend(batch['labels'].tolist())\n",
    "\n",
    "        if last_batch:\n",
    "            _logger.info(f'avg_accuracy : {acc_m.avg}')"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "  0%|          | 0/125 [00:00<?, ?it/s]/home/ubuntu/anaconda3/envs/sbse/lib/python3.7/site-packages/transformers/tokenization_utils_base.py:2190: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
      "  FutureWarning,\n",
      " 99%|█████████▉| 124/125 [00:30<00:00,  3.97it/s]avg_accuracy : 0.64000004529953\n",
      "100%|██████████| 125/125 [00:30<00:00,  4.05it/s]\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "source": [
    "_logger.info(classification_report(true_ids, pred_ids))"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.62      0.71      0.66       492\n",
      "           1       0.67      0.57      0.62       508\n",
      "\n",
      "    accuracy                           0.64      1000\n",
      "   macro avg       0.64      0.64      0.64      1000\n",
      "weighted avg       0.64      0.64      0.64      1000\n",
      "\n"
     ]
    }
   ],
   "metadata": {}
  }
 ],
 "metadata": {
  "orig_nbformat": 4,
  "language_info": {
   "name": "python",
   "version": "3.7.10",
   "mimetype": "text/x-python",
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "pygments_lexer": "ipython3",
   "nbconvert_exporter": "python",
   "file_extension": ".py"
  },
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3.7.10 64-bit ('sbse': conda)"
  },
  "interpreter": {
   "hash": "21f9bb8429c2ab361b1d9d2ec03ac7f28e3bdc6e965d8415bdce4abed407f189"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}